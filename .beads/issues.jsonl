{"id":"ragrec-22w","title":"Phase 5.1: HITL Validation \u0026 Benchmark System","description":"## Objective\nBuild a Human-in-the-Loop (HITL) validation system with benchmark datasets for quality assurance.\n\n## Tasks\n- [ ] Research and select benchmark dataset from HuggingFace\n  - Candidates: MerRec (Mercari), Amazon Reviews, MIND\n  - Need: ground truth relevance labels\n- [ ] Create src/ragrec/benchmark/__init__.py\n- [ ] Create src/ragrec/benchmark/datasets.py\n  - Load benchmark dataset from HuggingFace\n  - Extract query-product relevance pairs\n- [ ] Create src/ragrec/benchmark/metrics.py\n  - Implement nDCG@k\n  - Implement Precision@k\n  - Implement Recall@k\n  - Implement MRR (Mean Reciprocal Rank)\n- [ ] Create src/ragrec/benchmark/runner.py\n  - Run recommendations against benchmark queries\n  - Compare with ground truth\n  - Generate metrics report\n- [ ] Create Streamlit HITL validation page\n  - Display recommendation + ground truth side-by-side\n  - Allow human to rate relevance (1-5 scale)\n  - Track inter-annotator agreement\n  - Export annotations for retraining\n- [ ] Create n8n HITL workflow\n  - Schedule periodic validation batches\n  - Notify reviewers via Slack/email\n  - Collect and aggregate feedback\n  - Alert on quality degradation\n- [ ] Create benchmark comparison dashboard\n  - Track metrics over time\n  - Compare fusion strategies\n  - Compare vector stores\n- [ ] Add CLI command: ragrec benchmark --dataset \u003cname\u003e\n- [ ] Write benchmark tests\n\n## Benchmark Dataset Requirements\n- Product recommendations with relevance labels\n- Sufficient size (1000+ query-product pairs)\n- Available on HuggingFace for easy loading\n\n## HITL UI Features\n- Queue of items to validate\n- Side-by-side comparison view\n- Quick rating buttons (1-5 or thumbs up/down)\n- Skip option for unclear cases\n- Progress tracking\n- Export validated pairs as training data\n\n## n8n HITL Workflow\n- Trigger: scheduled or on-demand\n- Select random sample of recommendations\n- Push to validation queue\n- Notify assigned reviewers\n- Collect responses\n- Calculate agreement metrics\n- Store results for analysis\n- Alert if quality drops below threshold\n\n## Success Criteria\n- Benchmark dataset loaded and usable\n- nDCG@10 calculated for current system\n- HITL UI allows efficient validation\n- n8n workflow automates validation scheduling\n- Quality metrics tracked over time\n\n## Dependencies\n- ragrec-c8s (Phase 5: n8n Workflow Automation)","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-28T09:47:05.419131+01:00","updated_at":"2026-01-28T09:47:05.419131+01:00","dependencies":[{"issue_id":"ragrec-22w","depends_on_id":"ragrec-c8s","type":"blocks","created_at":"2026-01-28T09:47:50.867849+01:00","created_by":"daemon"}]}
{"id":"ragrec-39r","title":"Phase 4.1: Complete FastAPI Application","description":"## Objective\nComplete the FastAPI application with all endpoints and proper structure.\n\n## Tasks\n- [ ] Create src/ragrec/api/__init__.py\n- [ ] Create src/ragrec/api/main.py with app factory and lifespan\n- [ ] Create src/ragrec/api/dependencies.py for DI\n- [ ] Create src/ragrec/api/routers/recommend.py\n- [ ] Create src/ragrec/api/routers/similar.py\n- [ ] Create src/ragrec/api/routers/health.py\n- [ ] Create src/ragrec/api/routers/internal.py (for n8n)\n- [ ] Define Pydantic request/response models\n- [ ] Add proper error handling\n- [ ] Add request logging\n- [ ] Add OpenAPI documentation\n- [ ] Add CORS configuration\n- [ ] Write API tests\n\n## Endpoints Summary\n- POST /api/v1/recommend - personalized recommendations\n- POST /api/v1/similar - visual similarity\n- GET /api/v1/persona/{customer_id} - customer persona\n- GET /api/v1/health - health check\n- GET /api/v1/benchmark/compare - vector store comparison\n- POST /api/v1/internal/* - n8n endpoints\n\n## Success Criteria\n- All endpoints documented in OpenAPI\n- API tests pass\n- Error responses are consistent\n\n## Dependencies\n- ragrec-457 (Phase 4: Fusion \u0026 Ranking)","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-28T09:46:22.093566+01:00","updated_at":"2026-01-28T09:46:22.093566+01:00","dependencies":[{"issue_id":"ragrec-39r","depends_on_id":"ragrec-457","type":"blocks","created_at":"2026-01-28T09:47:50.046494+01:00","created_by":"daemon"}]}
{"id":"ragrec-422","title":"Phase 0.1: Sample Data Creation","description":"## Objective\nCreate sample dataset from H\u0026M data for development and testing.\n\n## Tasks\n- [ ] Create scripts/create_sample_data.py using Polars\n- [ ] Sample 1000 products with images available\n- [ ] Sample 500 customers who purchased these products\n- [ ] Sample 5000 transactions linking them\n- [ ] Save as Parquet files in data/sample/\n- [ ] Copy 100 sample product images to data/sample/images/\n- [ ] Create data/sample/README.md documenting the sample\n- [ ] Verify sample data loads correctly\n\n## Prerequisites\n- Download H\u0026M dataset from Kaggle to data/hm/\n\n## Success Criteria\n- data/sample/articles_sample.parquet exists (~1000 rows)\n- data/sample/customers_sample.parquet exists (~500 rows)\n- data/sample/transactions_sample.parquet exists (~5000 rows)\n- data/sample/images/ contains 100 JPG files\n\n## Dependencies\n- ragrec-zvc (Phase 0: Foundation)","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-28T09:44:47.008236+01:00","updated_at":"2026-01-28T10:47:44.714021+01:00","closed_at":"2026-01-28T10:47:44.714021+01:00","close_reason":"Sample data created successfully. Script creates 1,000 products, 4,946 customers, 5,000 transactions, 100 images from H\u0026M dataset. All saved as Parquet in data/sample/. Data verified loading correctly.","dependencies":[{"issue_id":"ragrec-422","depends_on_id":"ragrec-zvc","type":"blocks","created_at":"2026-01-28T09:47:38.1149+01:00","created_by":"daemon"}]}
{"id":"ragrec-457","title":"Phase 4: Fusion \u0026 Ranking Layer","description":"## Objective\nImplement multi-signal fusion to combine visual, persona, and graph recommendations.\n\n## Tasks\n- [ ] Create src/ragrec/recommender/fusion.py\n- [ ] Implement abstract FusionStrategy interface\n- [ ] Implement RRFFusion (Reciprocal Rank Fusion) - baseline\n- [ ] Implement WeightedFusion (from experimental/ranker.py)\n  - MinMax normalization per channel\n  - Configurable weights (w_visual, w_persona, w_graph)\n- [ ] Create src/ragrec/recommender/reranker.py\n- [ ] Implement CrossEncoderReranker (from experimental/ranker.py)\n  - Use cross-encoder/ms-marco-MiniLM-L-6-v2\n  - Rerank top-k candidates for final precision\n- [ ] Create src/ragrec/recommender/engine.py\n- [ ] Implement unified recommend(customer_id, top_k) method\n  - Fetch visual candidates\n  - Fetch persona candidates\n  - Fetch graph candidates\n  - Apply fusion strategy\n  - Optional: apply cross-encoder rerank\n- [ ] Add FastAPI endpoint: POST /api/v1/recommend\n- [ ] Add CLI command: ragrec recommend \u003ccustomer_id\u003e\n- [ ] Write integration tests\n\n## Technical Notes\n- Fusion strategy should be configurable\n- Log intermediate scores for debugging\n- Support A/B testing different strategies\n\n## Success Criteria\n- Full recommendation pipeline works end-to-end\n- Response time \u003c 500ms\n- Recommendations combine all three signals\n\n## Dependencies\n- ragrec-mvp (Phase 3.2: Persona-Based Recommendations)","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-28T09:46:14.116648+01:00","updated_at":"2026-01-28T09:46:14.116648+01:00","dependencies":[{"issue_id":"ragrec-457","depends_on_id":"ragrec-mvp","type":"blocks","created_at":"2026-01-28T09:47:49.845878+01:00","created_by":"daemon"}]}
{"id":"ragrec-4dm","title":"Phase 2: Neo4j Schema \u0026 Data Import","description":"## Objective\nSet up Neo4j graph database with product-customer-category relationships.\n\n## Tasks\n- [ ] Create src/ragrec/graph/__init__.py\n- [ ] Create src/ragrec/graph/client.py with async Neo4j client\n- [ ] Create src/ragrec/graph/schema.py with Cypher schema definitions\n- [ ] Define node types: Customer, Product, Category, Persona\n- [ ] Define relationships: PURCHASED, IN_CATEGORY, SIMILAR_TO, BELONGS_TO, PARENT_OF\n- [ ] Create src/ragrec/etl/graph_loader.py for bulk import\n- [ ] Implement bulk import using UNWIND for performance\n- [ ] Create category hierarchy from H\u0026M data (section -\u003e garment_group -\u003e product_type)\n- [ ] Create SIMILAR_TO edges based on visual similarity (top-5 per product)\n- [ ] Add constraints and indexes for performance\n- [ ] Write integration tests\n\n## Technical Notes\n- Use neo4j async driver\n- Batch imports (1000 nodes/relationships per transaction)\n- Create indexes on frequently queried properties\n\n## Success Criteria\n- Neo4j contains 1000 Product nodes (sample)\n- Neo4j contains 500 Customer nodes (sample)\n- Category hierarchy is complete\n- PURCHASED relationships match transactions table\n- Cypher queries return in \u003c100ms\n\n## Dependencies\n- ragrec-7u9 (Phase 1.3: Visual Similarity Search)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-28T09:45:30.91982+01:00","updated_at":"2026-01-28T12:35:48.372706+01:00","closed_at":"2026-01-28T12:35:48.372706+01:00","close_reason":"Closed","dependencies":[{"issue_id":"ragrec-4dm","depends_on_id":"ragrec-7u9","type":"blocks","created_at":"2026-01-28T09:47:48.796766+01:00","created_by":"daemon"}]}
{"id":"ragrec-66z","title":"Phase 2.1: Graph Queries \u0026 Recommendations","description":"## Objective\nImplement GraphRAG-based recommendation queries.\n\n## Tasks\n- [ ] Create src/ragrec/graph/queries.py with common query patterns\n- [ ] Implement get_product_neighborhood(product_id, depth)\n- [ ] Implement get_customer_journey(customer_id)\n- [ ] Implement get_collaborative_recommendations(customer_id, top_k)\n  - Products purchased by similar customers\n  - Products similar to customer's purchases\n- [ ] Implement get_category_recommendations(category_id, top_k)\n- [ ] Create src/ragrec/recommender/collaborative.py\n- [ ] Combine graph traversal with vector similarity\n- [ ] Add FastAPI endpoint: GET /api/v1/graph/neighborhood/{product_id}\n- [ ] Write integration tests\n\n## Technical Notes\n- Use parameterized Cypher queries (prevent injection)\n- Limit traversal depth to prevent explosion\n- Cache frequently accessed paths\n\n## Success Criteria\n- Product neighborhood returns related products via graph\n- Customer journey shows purchase timeline\n- Collaborative recommendations differ from pure visual similarity\n\n## Dependencies\n- ragrec-4dm (Phase 2: Neo4j Schema)","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-28T09:45:39.636656+01:00","updated_at":"2026-01-28T09:45:39.636656+01:00","dependencies":[{"issue_id":"ragrec-66z","depends_on_id":"ragrec-4dm","type":"blocks","created_at":"2026-01-28T09:47:49.008067+01:00","created_by":"daemon"}]}
{"id":"ragrec-7u9","title":"Phase 1.3: Visual Similarity Search","description":"## Objective\nImplement end-to-end visual similarity search feature.\n\n## Tasks\n- [ ] Create src/ragrec/recommender/__init__.py\n- [ ] Create src/ragrec/recommender/visual.py\n- [ ] Implement find_similar(image_bytes, top_k) -\u003e list of products\n- [ ] Implement find_similar_by_id(product_id, top_k) -\u003e list of products\n- [ ] Add category filtering option\n- [ ] Add price range filtering option\n- [ ] Create FastAPI endpoint: POST /api/v1/similar\n- [ ] Accept image upload (multipart/form-data)\n- [ ] Return ProductScore response model\n- [ ] Add CLI command: ragrec similar \u003cimage_path\u003e\n- [ ] Write integration tests\n\n## Success Criteria\n- Upload product image -\u003e get 10 visually similar products\n- Response time \u003c 200ms\n- Results are visually coherent (same category, similar style)\n\n## Dependencies\n- ragrec-a22 (Phase 1.2: pgvector Integration)","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-28T09:45:21.369857+01:00","updated_at":"2026-01-28T11:25:17.669856+01:00","closed_at":"2026-01-28T11:25:17.669856+01:00","close_reason":"Visual similarity search complete. Implemented VisualRecommender with find_similar() and find_similar_by_id() methods. Created FastAPI endpoint POST /api/v1/similar with image upload support. Added CLI commands: 'ragrec search \u003cimage\u003e' and 'ragrec similar \u003cid\u003e'. Category filtering supported. All 12 integration tests passing. Query time ~200ms (warm model), meets performance target.","dependencies":[{"issue_id":"ragrec-7u9","depends_on_id":"ragrec-a22","type":"blocks","created_at":"2026-01-28T09:47:48.584582+01:00","created_by":"daemon"}]}
{"id":"ragrec-8m6","title":"DevOps: MacBook sync infrastructure","description":"Created automated sync scripts for transferring RagRec development environment from M2 Pro to MacBook.\n\n**Files Created:**\n- scripts/sync_to_macbook.sh - Syncs code + database from M2 Pro\n- scripts/restore_database.sh - Restores database on MacBook\n- Serena memory: macbook-sync-process.md\n\n**Features:**\n- Quick sync mode: code + 11MB database dump (30 seconds)\n- Full sync mode (--full): includes 32GB H\u0026M dataset (15-30 min)\n- Automated git push + beads sync\n- PostgreSQL dump creation and transfer\n- Verification steps\n\n**Usage:**\n```bash\n# Quick sync\n./scripts/sync_to_macbook.sh\n\n# Full sync\n./scripts/sync_to_macbook.sh --full\n\n# Restore on MacBook\nssh macbook 'cd ~/demos/ragrec \u0026\u0026 ./scripts/restore_database.sh'\n```\n\n**Status:** Tested and working. All 14 tests pass on MacBook after sync.","status":"closed","priority":4,"issue_type":"task","created_at":"2026-01-28T12:16:16.990208+01:00","updated_at":"2026-01-28T12:16:33.41636+01:00","closed_at":"2026-01-28T12:16:33.41636+01:00","close_reason":"MacBook sync infrastructure complete. Created sync_to_macbook.sh (M2 Pro â†’ MacBook transfer) and restore_database.sh (MacBook DB restore). Documented in Serena memory. Tested successfully - all 14 tests pass on MacBook after sync."}
{"id":"ragrec-9m9","title":"Phase 6: Qdrant Integration \u0026 Benchmarking","description":"## Objective\nAdd Qdrant as alternative vector store and benchmark against pgvector.\n\n## Tasks\n- [ ] Create scripts/setup_qdrant.sh\n- [ ] Create src/ragrec/vectorstore/qdrant.py implementing VectorStore interface\n- [ ] Implement upsert, search, hybrid_search for Qdrant\n- [ ] Configure HNSW with same parameters as pgvector\n- [ ] Create experimental/benchmarks/vectorstore_comparison.py\n- [ ] Benchmark metrics:\n  - Latency (p50, p95, p99)\n  - Throughput (QPS)\n  - Recall@10 vs ground truth\n  - Memory usage\n- [ ] Run benchmark with 1000 queries on sample data\n- [ ] Generate comparison report (Polars DataFrame -\u003e markdown)\n- [ ] Add n8n workflow for weekly benchmark\n- [ ] Document findings and recommendation\n\n## Success Criteria\n- Both vector stores pass same test suite\n- Benchmark report generated\n- Clear recommendation for production use\n\n## Dependencies\n- ragrec-22w (Phase 5.1: HITL Validation)","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-28T09:47:13.923453+01:00","updated_at":"2026-01-28T09:47:13.923453+01:00","dependencies":[{"issue_id":"ragrec-9m9","depends_on_id":"ragrec-22w","type":"blocks","created_at":"2026-01-28T09:47:51.108274+01:00","created_by":"daemon"}]}
{"id":"ragrec-a22","title":"Phase 1.2: pgvector Integration","description":"## Objective\nImplement pgvector-based vector store for similarity search.\n\n## Tasks\n- [ ] Create src/ragrec/vectorstore/__init__.py\n- [ ] Create src/ragrec/vectorstore/base.py with abstract VectorStore interface\n- [ ] Create src/ragrec/vectorstore/pgvector.py implementation\n- [ ] Implement upsert(ids, embeddings, metadata)\n- [ ] Implement search(query_embedding, top_k, filters)\n- [ ] Implement hybrid_search(query_embedding, query_text, top_k)\n- [ ] Create HNSW index on embedding column\n- [ ] Configure index parameters (ef_construction=128, m=16)\n- [ ] Add cosine distance operator (\u003c=\u003e)\n- [ ] Write integration tests with real PostgreSQL\n\n## Technical Notes\n- Use asyncpg with pgvector extension\n- Return results as Polars DataFrame\n- Support metadata filtering in WHERE clause\n\n## Success Criteria\n- Vector similarity search returns results in \u003c50ms for 1000 products\n- Top-10 results for a query image are visually similar\n- HNSW index is created and used (check EXPLAIN)\n\n## Dependencies\n- ragrec-sed (Phase 1.1: SigLIP Embeddings)","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-28T09:45:14.244351+01:00","updated_at":"2026-01-28T11:11:07.21522+01:00","closed_at":"2026-01-28T11:11:07.21522+01:00","close_reason":"pgvector similarity search complete. Implemented PgVectorStore with HNSW index (m=16, ef_construction=128). Query time ~1ms (\u003c 50ms target). CLI commands working: 'ragrec create-index' and 'ragrec search \u003cimage\u003e'. Hybrid search implemented. All 5 integration tests passing.","dependencies":[{"issue_id":"ragrec-a22","depends_on_id":"ragrec-sed","type":"blocks","created_at":"2026-01-28T09:47:48.360836+01:00","created_by":"daemon"}]}
{"id":"ragrec-c8s","title":"Phase 5: n8n Workflow Automation","description":"## Objective\nSet up n8n workflows for ETL, real-time events, and scheduled jobs.\n\n## Tasks\n- [ ] Create n8n/workflows/ directory structure\n- [ ] Create ETL workflows:\n  - daily_product_sync.json\n  - weekly_full_reindex.json\n  - import_hm_dataset.json\n- [ ] Create real-time workflows:\n  - new_product_webhook.json\n  - purchase_event.json\n  - product_update_webhook.json\n- [ ] Create persona workflows:\n  - daily_persona_refresh.json\n  - weekly_persona_discovery.json\n  - persona_drift_alert.json\n- [ ] Create benchmark workflows:\n  - weekly_vectorstore_bench.json\n- [ ] Create integration workflows:\n  - slack_alerts.json\n  - email_persona_report.json\n- [ ] Configure n8n credentials (gitignored)\n- [ ] Add internal API authentication for n8n\n- [ ] Document workflow import process\n- [ ] Test each workflow manually\n\n## Success Criteria\n- Workflows importable into n8n\n- Daily persona refresh runs on schedule\n- Webhooks trigger correctly\n- Slack alerts work for errors\n\n## Dependencies\n- ragrec-vx3 (Phase 4.3: CLI Application)","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-28T09:46:49.026085+01:00","updated_at":"2026-01-28T09:46:49.026085+01:00","dependencies":[{"issue_id":"ragrec-c8s","depends_on_id":"ragrec-vx3","type":"blocks","created_at":"2026-01-28T09:47:50.664284+01:00","created_by":"daemon"}]}
{"id":"ragrec-d7b","title":"Phase 3: Customer Behavior Embeddings","description":"## Objective\nGenerate embeddings from customer purchase history for persona discovery.\n\n## Tasks\n- [ ] Create src/ragrec/embeddings/sequence.py\n- [ ] Implement purchase sequence encoding\n  - Input: list of product_ids in chronological order\n  - Output: 256-dim behavior embedding\n- [ ] Options for encoding:\n  - Average of purchased product embeddings (simple baseline)\n  - Transformer-based sequence encoder (advanced)\n- [ ] Add recency weighting (recent purchases weighted more)\n- [ ] Store behavior embeddings in customers table\n- [ ] Add CLI command: ragrec generate-embeddings --customers\n- [ ] Write unit tests\n\n## Technical Notes\n- Start with weighted average approach (simpler)\n- Consider transformer if baseline insufficient\n- Handle customers with few purchases gracefully\n\n## Success Criteria\n- All customers have behavior_embedding populated\n- Similar customers (by purchases) have similar embeddings\n- Embedding generation completes in reasonable time\n\n## Dependencies\n- ragrec-66z (Phase 2.1: Graph Queries)","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-28T09:45:47.781875+01:00","updated_at":"2026-01-28T09:45:47.781875+01:00","dependencies":[{"issue_id":"ragrec-d7b","depends_on_id":"ragrec-66z","type":"blocks","created_at":"2026-01-28T09:47:49.218439+01:00","created_by":"daemon"}]}
{"id":"ragrec-dmu","title":"Phase 4.2: Streamlit Demo UI","description":"## Objective\nBuild Streamlit demo application for stakeholder demos.\n\n## Tasks\n- [ ] Create ui/app.py with navigation\n- [ ] Create ui/pages/1_visual_search.py\n  - Image upload widget\n  - Sample product selector\n  - Results grid with images, names, scores\n  - Category filter multiselect\n- [ ] Create ui/pages/2_persona_demo.py\n  - Persona overview cards\n  - Customer lookup input\n  - Recommendations display\n  - UMAP embedding visualization (plotly)\n- [ ] Create ui/pages/3_graph_explorer.py\n  - Query type selector\n  - Product/customer ID input\n  - Interactive pyvis graph\n  - Statistics display\n- [ ] Create ui/components.py for shared widgets\n- [ ] Add CLI command: ragrec ui\n- [ ] Style with consistent theme\n\n## Success Criteria\n- All three pages functional\n- Can upload image and see similar products\n- Can explore customer personas\n- Can visualize graph neighborhoods\n\n## Dependencies\n- ragrec-39r (Phase 4.1: Complete FastAPI)","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-28T09:46:30.473484+01:00","updated_at":"2026-01-28T09:46:30.473484+01:00","dependencies":[{"issue_id":"ragrec-dmu","depends_on_id":"ragrec-39r","type":"blocks","created_at":"2026-01-28T09:47:50.248786+01:00","created_by":"daemon"}]}
{"id":"ragrec-gb0","title":"Phase 6.2: Rust Optimization Candidates","description":"## Objective\nIdentify and document hot paths for future Rust optimization.\n\n## Tasks\n- [ ] Profile full recommendation pipeline with py-spy\n- [ ] Identify top 5 CPU-intensive functions\n- [ ] Document Rust migration candidates:\n  - Embedding inference (candle/ort)\n  - PCA compression (ndarray + PyO3)\n  - Batch vector operations (faer/nalgebra)\n  - Polars UDFs (pyo3-polars)\n  - Custom similarity functions\n- [ ] Create proof-of-concept for highest-impact candidate\n- [ ] Benchmark Python vs Rust implementation\n- [ ] Document expected vs actual speedup\n- [ ] Create migration plan for Phase 7+\n\n## Rust Migration Candidates Table\n| Component | Current | Rust Option | Expected Speedup |\n|-----------|---------|-------------|------------------|\n| Embedding inference | transformers | candle/ort | 2-5x |\n| PCA compression | sklearn | ndarray + PyO3 | 3-10x |\n| Batch vector ops | numpy | faer/nalgebra | 2-5x |\n| Polars UDFs | Python | pyo3-polars | 5-20x |\n\n## Success Criteria\n- Profile report generated\n- Top bottlenecks identified\n- At least one Rust PoC working\n- Migration plan documented\n\n## Dependencies\n- ragrec-z14 (Phase 6.1: Experimental Ranker Evaluation)","status":"open","priority":4,"issue_type":"task","created_at":"2026-01-28T09:47:33.178654+01:00","updated_at":"2026-01-28T09:47:33.178654+01:00","dependencies":[{"issue_id":"ragrec-gb0","depends_on_id":"ragrec-z14","type":"blocks","created_at":"2026-01-28T09:47:51.52708+01:00","created_by":"daemon"}]}
{"id":"ragrec-mvp","title":"Phase 3.2: Persona-Based Recommendations","description":"## Objective\nUse personas to personalize recommendations.\n\n## Tasks\n- [ ] Create src/ragrec/personas/matching.py\n- [ ] Implement get_customer_persona(customer_id)\n- [ ] Implement get_persona_recommendations(persona_id, top_k)\n  - Products popular within persona\n  - Weighted by category affinity\n- [ ] Create persona weight vectors (category -\u003e weight mapping)\n- [ ] Integrate persona into recommendation engine\n- [ ] Add FastAPI endpoint: GET /api/v1/persona/{customer_id}\n- [ ] Write integration tests\n\n## Success Criteria\n- Customers in same persona get similar recommendations\n- Recommendations reflect persona preferences (categories, price points)\n- Persona lookup is fast (\u003c50ms)\n\n## Dependencies\n- ragrec-qw9 (Phase 3.1: Hybrid Persona Discovery)","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-28T09:46:03.805821+01:00","updated_at":"2026-01-28T09:46:03.805821+01:00","dependencies":[{"issue_id":"ragrec-mvp","depends_on_id":"ragrec-qw9","type":"blocks","created_at":"2026-01-28T09:47:49.636859+01:00","created_by":"daemon"}]}
{"id":"ragrec-qw9","title":"Phase 3.1: Hybrid Persona Discovery","description":"## Objective\nDiscover customer personas using hybrid approach (embeddings + graph communities).\n\n## Tasks\n- [ ] Create src/ragrec/personas/__init__.py\n- [ ] Create src/ragrec/personas/discovery.py\n- [ ] Implement embedding clustering with HDBSCAN\n  - Input: customer behavior embeddings\n  - Output: cluster assignments\n- [ ] Implement graph community detection with Neo4j\n  - Run Louvain algorithm on customer-product bipartite graph\n  - Extract community assignments\n- [ ] Implement hybrid fusion\n  - Persona = intersection of embedding cluster + graph community\n  - Handle edge cases (customers in multiple communities)\n- [ ] Create src/ragrec/personas/models.py with Persona dataclass\n- [ ] Generate persona descriptions (top categories, avg age, avg spend)\n- [ ] Store personas in Neo4j as Persona nodes\n- [ ] Create BELONGS_TO relationships\n- [ ] Add CLI command: ragrec discover-personas\n- [ ] Write unit tests\n\n## Success Criteria\n- 6-10 distinct personas discovered\n- Each persona has interpretable characteristics\n- Customers assigned to exactly one persona\n- Persona distribution is reasonable (no single dominant persona)\n\n## Dependencies\n- ragrec-d7b (Phase 3: Customer Behavior Embeddings)","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-28T09:45:56.30169+01:00","updated_at":"2026-01-28T09:45:56.30169+01:00","dependencies":[{"issue_id":"ragrec-qw9","depends_on_id":"ragrec-d7b","type":"blocks","created_at":"2026-01-28T09:47:49.433183+01:00","created_by":"daemon"}]}
{"id":"ragrec-sed","title":"Phase 1.1: SigLIP Embedding Generation","description":"## Objective\nImplement SigLIP-based embedding generation for product images and text.\n\n## Tasks\n- [ ] Create src/ragrec/embeddings/__init__.py\n- [ ] Create src/ragrec/embeddings/base.py with abstract Embedder interface\n- [ ] Create src/ragrec/embeddings/siglip.py implementing SigLIP encoder\n- [ ] Load google/siglip-base-patch16-256-multilingual model\n- [ ] Implement encode_image(image_bytes) -\u003e 768-dim vector\n- [ ] Implement encode_text(text) -\u003e 768-dim vector\n- [ ] Implement batch_encode_images() for efficiency\n- [ ] Configure for Apple Metal (MPS) device\n- [ ] Add optional PCA compression (768 -\u003e 128 dims)\n- [ ] Add CLI command: ragrec generate-embeddings\n- [ ] Store embeddings in products table (image_embedding, text_embedding columns)\n- [ ] Write unit tests\n\n## Technical Notes\n- Use transformers library for SigLIP\n- Device: mps (Apple Metal) for M2 Pro\n- Batch size: 32 (configurable)\n\n## Success Criteria\n- uv run ragrec generate-embeddings completes for sample data\n- Products table has non-null image_embedding for all rows\n- Embedding dimension is 768 (or 128 if PCA enabled)\n- GPU utilization visible during generation\n\n## Dependencies\n- ragrec-xbz (Phase 1: H\u0026M Data Loader)","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-28T09:45:05.600166+01:00","updated_at":"2026-01-28T11:06:52.022511+01:00","closed_at":"2026-01-28T11:06:52.022511+01:00","close_reason":"SigLIP embedding generation complete. Implemented visual embedder with Apple Metal (MPS) GPU acceleration. Generated 1,000 embeddings (768-dim) and stored in PostgreSQL with pgvector. CLI command working: 'ragrec generate-embeddings data/sample/images'. All embeddings verified in database.","dependencies":[{"issue_id":"ragrec-sed","depends_on_id":"ragrec-xbz","type":"blocks","created_at":"2026-01-28T09:47:48.16093+01:00","created_by":"daemon"}]}
{"id":"ragrec-vx3","title":"Phase 4.3: CLI Application (Typer)","description":"## Objective\nComplete the CLI application for batch operations.\n\n## Tasks\n- [ ] Create cli/__init__.py\n- [ ] Create cli/main.py with Typer app\n- [ ] Implement all CLI commands:\n  - load-data \u003csource\u003e [--sample]\n  - generate-embeddings [--batch-size] [--products] [--customers]\n  - discover-personas [--n-clusters] [--min-customers]\n  - recommend \u003ccustomer_id\u003e [--top-k] [--output]\n  - similar \u003cimage_path\u003e [--top-k]\n  - benchmark [--vectorstore] [--queries]\n  - serve [--host] [--port] [--reload]\n  - ui [--port]\n- [ ] Add progress bars for long operations\n- [ ] Add --verbose flag for debugging\n- [ ] Add --json output format option\n- [ ] Update pyproject.toml with CLI entry point\n- [ ] Write CLI tests\n\n## Success Criteria\n- All commands documented with --help\n- Progress visible during long operations\n- JSON output parseable by scripts\n\n## Dependencies\n- ragrec-dmu (Phase 4.2: Streamlit UI)","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-28T09:46:39.506233+01:00","updated_at":"2026-01-28T09:46:39.506233+01:00","dependencies":[{"issue_id":"ragrec-vx3","depends_on_id":"ragrec-dmu","type":"blocks","created_at":"2026-01-28T09:47:50.457947+01:00","created_by":"daemon"}]}
{"id":"ragrec-xbz","title":"Phase 1: H\u0026M Data Loader (Polars ETL)","description":"## Objective\nBuild ETL pipeline to load H\u0026M dataset into PostgreSQL using Polars.\n\n## Tasks\n- [ ] Create src/ragrec/etl/__init__.py\n- [ ] Create src/ragrec/etl/hm_loader.py with Polars\n- [ ] Define PostgreSQL schema (products, customers, transactions tables)\n- [ ] Implement load_articles() - parse articles.csv, transform, bulk insert\n- [ ] Implement load_customers() - parse customers.csv, create age_bracket enum\n- [ ] Implement load_transactions() - parse transactions.csv with proper types\n- [ ] Add CLI command: ragrec load-data\n- [ ] Support --sample flag to load only sample data\n- [ ] Add progress logging with loguru\n- [ ] Write unit tests for transformations\n\n## Technical Notes\n- Use Polars (NOT Pandas) throughout\n- Use asyncpg for async database operations\n- Batch inserts (1000 rows per batch)\n\n## Success Criteria\n- uv run ragrec load-data ./data/sample --sample completes\n- Products table has 1000 rows\n- Customers table has 500 rows\n- Transactions table has 5000 rows\n\n## Dependencies\n- ragrec-zvc (Phase 0: Foundation)\n- ragrec-422 (Phase 0.1: Sample Data)","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-28T09:44:56.589663+01:00","updated_at":"2026-01-28T10:51:23.40334+01:00","closed_at":"2026-01-28T10:51:23.40334+01:00","close_reason":"ETL pipeline complete. Polars-based loader with async PostgreSQL inserts. Schema created with products, customers, transactions tables. CLI command working: 'ragrec load-data data/sample'. All 1,000 products, 4,946 customers, 5,000 transactions loaded successfully. Tests passing (7/7).","dependencies":[{"issue_id":"ragrec-xbz","depends_on_id":"ragrec-422","type":"blocks","created_at":"2026-01-28T09:47:47.957881+01:00","created_by":"daemon"}]}
{"id":"ragrec-z14","title":"Phase 6.1: Experimental Ranker Evaluation","description":"## Objective\nSystematically evaluate ideas from experimental/ranker.py.\n\n## Tasks\n- [ ] Create experimental/benchmarks/fusion_comparison.py\n  - Compare RRF vs Weighted Merge\n  - Test different weight configurations\n- [ ] Create experimental/benchmarks/reranker_impact.py\n  - Measure improvement from cross-encoder rerank\n  - Measure latency cost\n- [ ] Create experimental/benchmarks/graph_backend.py\n  - Compare NetworkX (in-memory) vs Neo4j\n  - Test for different dataset sizes\n- [ ] Run all benchmarks on sample data\n- [ ] Document results in experimental/RESULTS.md\n- [ ] Update fusion.py with winning strategy\n- [ ] Profile hot paths with py-spy\n\n## Decision Matrix\n| Approach | nDCG@10 | Latency | Keep? |\n|----------|---------|---------|-------|\n| RRF | TBD | TBD | ? |\n| Weighted | TBD | TBD | ? |\n| +CrossEncoder | TBD | TBD | ? |\n\n## Success Criteria\n- All experimental approaches benchmarked\n- Clear winner identified for each component\n- Results documented\n- Hot paths identified for future Rust optimization\n\n## Dependencies\n- ragrec-9m9 (Phase 6: Qdrant Integration)","status":"open","priority":3,"issue_type":"task","created_at":"2026-01-28T09:47:23.811769+01:00","updated_at":"2026-01-28T09:47:23.811769+01:00","dependencies":[{"issue_id":"ragrec-z14","depends_on_id":"ragrec-9m9","type":"blocks","created_at":"2026-01-28T09:47:51.322439+01:00","created_by":"daemon"}]}
{"id":"ragrec-zvc","title":"Phase 0: Project Foundation \u0026 Scaffolding","description":"## Objective\nSet up the complete project foundation with all services running.\n\n## Tasks\n- [ ] Create pyproject.toml with all dependencies\n- [ ] Create src/ragrec/ package structure\n- [ ] Create CLAUDE.md project context\n- [ ] Create MANIFESTO.md development rules\n- [ ] Set up scripts/setup_native.sh (Homebrew install for PostgreSQL, pgvector, Neo4j, n8n)\n- [ ] Configure PostgreSQL with pgvector extension\n- [ ] Verify Neo4j is accessible at localhost:7474\n- [ ] Create .env.example with all configuration variables\n- [ ] Create .gitignore (include data/hm/, data/embeddings/, .env)\n- [ ] Create basic FastAPI health endpoint\n- [ ] Create Makefile with common targets\n- [ ] Verify all services start correctly\n\n## Success Criteria\n- make setup completes without errors\n- make start starts PostgreSQL and Neo4j\n- curl localhost:8000/api/v1/health returns OK\n- pgvector extension verified\n\n## Dependencies\nNone - this is the foundation phase.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-28T09:44:39.195648+01:00","updated_at":"2026-01-28T10:08:21.999309+01:00","closed_at":"2026-01-28T10:08:21.999309+01:00","close_reason":"Phase 0 foundation complete. All core infrastructure in place: package structure, API with health endpoint (verified on port 9010), Makefile, setup scripts, MANIFESTO.md, tests passing. PostgreSQL setup scripted (user runs 'make setup'). Neo4j verified running. Port range 9010-9019 allocated and documented in ~/portranges_map.yaml."}
